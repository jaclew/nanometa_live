# This workflow handles the file processing from gzipped nanopore batch 
# files to a number of cumulative files used for the visualization.

configfile: "config.yaml"

import os
import pkg_resources
import time

# A check to see if there are any input files, before the workflow is run.
# Some rules hang the workflow if there are no files yet produced.
# First checks if the input dir exists:
while not os.path.isdir(config["nanopore_output_directory"]):
	print(str(config["nanopore_output_directory"]) + " not found. Waiting...")
	time.sleep(config["check_intervals_seconds"])
# Then checks if there are files in the dir:
while len(os.listdir(config["nanopore_output_directory"])) == 0:
	print("No files found in " + str(config["nanopore_output_directory"]) + ". Waiting...")
	time.sleep(config["check_intervals_seconds"]) 

# As soon as there are files in the dir:
print("Files found in " + str(config["nanopore_output_directory"]) + ". Starting workflow...")

def create_validation_placeholders():
	# Creates placeholder files for each of the 
	# species of interest specified in the config file.
	# This creates a starting point for the validation rules. 
	path1 = os.path.join(config["main_dir"], "validation_fastas") 
	path2 = os.path.join(config["main_dir"], "validation_fastas/placeholders")
	# full path could not be created in one go
	if not os.path.isdir(path1):
		os.mkdir(path1)
		print(path1 + ' - CREATED')
	if not os.path.isdir(path2):
		os.mkdir(path2)
		print(path2 + ' - CREATED')
	# Uses config file variable to create one file for each species.
	for species_id in config["species_of_interest"]:
		file_name = str(species_id)
		file_path = os.path.join(path2, file_name) 
		if not os.path.isfile(file_path):
			with open(file_path, 'w') as f:
				f.write("placeholder file")
				print(file_path + ' - CREATED')
	return path2

# Path used to create a list for validation using wildcards.
if config["blast_validation"] == True: # skips if validation=False
	valid_path = create_validation_placeholders()

# Creates a list of all files in the nanopore output folder, 
# to be used as list of input files for the workflow with the expand() function.
files = glob_wildcards(os.path.join(config["nanopore_output_directory"], "{file}.fastq.gz")).file

# Another similar list for the species of interest IDs.
if config["blast_validation"] == True: # skips if validation=False
	ids = glob_wildcards(os.path.join(valid_path, "{id}")).id

rule all:
	# "Rule all" specifies a rule that will be the default snake target.
	# The final files that need to be produced by the workflow are
	# input files of this rule, making snakemake find the rules with these
	# files as output, and proceed from there.  
	input:
		os.path.join(config["main_dir"], "kraken_cumul/kraken_cumul_txt.kraken2"),	# cumulative kraken txt file, used for validation
		os.path.join(config["main_dir"], "kraken_cumul/kraken_cumul_report.kreport2"),	# cumulative kraken report file, used in GUI
		os.path.join(config["main_dir"], "qc_data/cumul_qc.txt"),			# cumulative csv file containing qc data, used in GUI
		os.path.join(config["main_dir"], "fastp_reports/compiled_fastp.txt"),	# cumulative csv file with filter data for GUI
		# Target for the validation step, skipped if validation=False:
		os.path.join(config["main_dir"], "validation_fastas/force_validation.txt") if config["blast_validation"] else [],
		# Similar target for blast, skipped if validation=False:
		os.path.join(config["main_dir"], "blast_result_files/force_blast.txt") if config["blast_validation"] else []
	
rule extract_qc_info:
	# Extracts qc data:
	# Uses a py script to get the time of when the nanopore file was created, 
	# and gets info on nr of seqs and bp from the file.
	input:
		os.path.join(config["nanopore_output_directory"], "{fileName}.fastq.gz")	
	output:
		os.path.join(config["main_dir"], "qc_data/{fileName}.txt")
	conda:
		pkg_resources.resource_filename('nanometa_live', 'snakemake_envs/qc_env.yaml')
	shell:
		"python " + pkg_resources.resource_filename('nanometa_live', 'snakemake_scripts/qc_file_creator.py') + " {input} {output}"

rule combine_qc:
	# Combines all the individual qc files to one cumulative file.
	# expand() function needed when output is a single file.
	input:
		expand(os.path.join(config["main_dir"], "qc_data/{fileName}.txt"), fileName = files) 
	output:
		os.path.join(config["main_dir"], "qc_data/cumul_qc.txt")
	shell:
		"cat {input} > {output}"

rule fastp_filtering:
	# Filters the fastq files before Kraken classification. 
	# The filtering settings are per fastP defaults plus added low complexity filter.
	# "dev/null" is a workaround to discard the html report.
	# This sends the report into Linux limbo: dev/null.
	input:
		os.path.join(config["nanopore_output_directory"], "{fileName}.fastq.gz")
	output:
		fastqs = os.path.join(config["main_dir"], "fastp_filtered/{fileName}.fastq.gz"),
		report = os.path.join(config["main_dir"], "fastp_reports/{fileName}.json")
	conda:
		pkg_resources.resource_filename('nanometa_live', 'snakemake_envs/fastp_env.yaml')
	shell:
		"fastp -i {input} -o {output.fastqs} -y --json {output.report} --html /dev/null"

rule extract_fastp_info:
	# Parses the json fastP report files to extract the needed info.
	# Creates a txt/csv file with one line containing all info for the batch.
	input:
		os.path.join(config["main_dir"], "fastp_reports/{fileName}.json")
	output:
		os.path.join(config["main_dir"], "fastp_reports/{fileName}.txt")
	shell:
		"python " + pkg_resources.resource_filename('nanometa_live', 'snakemake_scripts/fastp_report_parser.py') + " {input} {output}"

rule combine_fastp:
	# Compiles all the filtering data into one file for the GUI.
	input:
		expand(os.path.join(config["main_dir"], "fastp_reports/{fileName}.txt"), fileName = files)
	output:
		os.path.join(config["main_dir"], "fastp_reports/compiled_fastp.txt")
	shell:
		"cat {input} > {output}"

rule run_kraken:
	# Kraken classification of filtered fastq files.
	# Produces a txt file and a kraken report for each fastq.
	# Argument --memory-mapping disables the kraken database being read into RAM. Slower, but less resource intense.
	# Also removes the fastP filtered fastq file when done to save space.
	input:
		os.path.join(config["main_dir"], "fastp_filtered/{fileName}.fastq.gz")
	output:
		txt = os.path.join(config["main_dir"], "kraken_results/{fileName}.kraken2"),
		report = os.path.join(config["main_dir"], "kraken_results/{fileName}.kreport2")
	threads: config["kraken_cores"] # more cores = faster analysis
	conda:
		pkg_resources.resource_filename('nanometa_live', 'snakemake_envs/kraken2_env.yaml')
	shell:
		"kraken2 --db " + config["kraken_db"] + " " + config["kraken_memory_mapping"] + " --gzip-compressed --report {output.report} {input} > {output.txt} && rm {input}"

rule append_kraken_cumul:
	# Concats all Kraken txt files into one long list, containing all results.
	# The cumulative txt file is used for extracting the validation sequences.
	input:
		expand(os.path.join(config["main_dir"], "kraken_results/{fileName}.kraken2"), fileName = files)
	output: 
		os.path.join(config["main_dir"], "kraken_cumul/kraken_cumul_txt.kraken2")
	shell:
		"cat {input} > {output}"

rule combine_kreports:
	# Uses combine_kreports.py script from KrakenTools to combine all individual
	# kreports to one.
	# Arguments: "no-headers" & "only-combined" should make the cumulative kreport identical to 
	# a standard kreport.
	# This step will hang the workflow if there are no files in the nanopore output folder,
	# hence the while loop at the beginning of the script.
	input: 
		expand(os.path.join(config["main_dir"], "kraken_results/{fileName}.kreport2"), fileName = files)
	output:
		os.path.join(config["main_dir"], "kraken_cumul/kraken_cumul_report.kreport2")
	conda:
		pkg_resources.resource_filename('nanometa_live', 'snakemake_envs/combine_kreports_env.yaml')
	shell:
		"python " +  pkg_resources.resource_filename('nanometa_live', 'snakemake_scripts/combine_kreports.py') + " --no-headers --only-combined -r {input} -o {output}" 

rule concat_fastqs:
	# Combines all fastqs into one for validation.
	input:
		expand(os.path.join(config["nanopore_output_directory"], "{fileName}.fastq.gz"), fileName = files)
	output:
		os.path.join(config["main_dir"], "validation_fastas/combined_fastqs.fastq.gz")
	shell:
		"cat {input} > {output}"
				
rule extract_validation_seqs:
	# KrakenTools extracts seqs belonging to each species of interest for validation.
	# The shell command transforms the placeholder
	# file names into species IDs to input into krakentools. 
	input:
		kraken_txt = os.path.join(config["main_dir"], "kraken_cumul/kraken_cumul_txt.kraken2"),
		fastqs = os.path.join(config["main_dir"], "validation_fastas/combined_fastqs.fastq.gz"),
		valids = os.path.join(config["main_dir"], "validation_fastas/placeholders/{ID}")
	output:
		os.path.join(config["main_dir"], "validation_fastas/valid_seqs_{ID}.fasta")
	threads: config["validation_cores"]
	conda:
		pkg_resources.resource_filename('nanometa_live', 'snakemake_envs/extract_validation_env.yaml')
	shell:
		"python " + pkg_resources.resource_filename('nanometa_live', 'snakemake_scripts/extract_kraken_reads.py') + " -k {input.kraken_txt} -s {input.fastqs} -o {output} --taxid $(basename {input.valids})"
		
rule force_validation:
	# This part creates a file which sole purpose is to 
	# force snakemake to iterate through the placeholder files representing
	# the species of interest IDs.
	# Also removes the concatenated fastq file to save space.
	input:	# ID variable needs to be skipped if validation=False
		id_names = expand(os.path.join(config["main_dir"], "validation_fastas/valid_seqs_{ID}.fasta"), ID = ids) if config["blast_validation"] else [],
		fastq = os.path.join(config["main_dir"], "validation_fastas/combined_fastqs.fastq.gz")
	output:
		os.path.join(config["main_dir"], "validation_fastas/force_validation.txt")
	shell:
		"""
		for file in {input.id_names};
		do
		echo $(basename "$file") >> {output};
		done
		rm -f {input.fastq}
		"""

rule run_blast:
	# BLASTS all sequences belonging to each species of interest against their
	# reference genome.
	# Specifies the BLAST database using the file names created by
	# the build_blast_db.py script.
	input:
		id_fasta = os.path.join(config["main_dir"], "validation_fastas/valid_seqs_{ID}.fasta"),
		database = os.path.join(config["main_dir"], "blast_databases/{ID}.fasta.nsq")
	output:
		os.path.join(config["main_dir"], "blast_result_files/{ID}.txt")
	threads: config["blast_cores"]
	conda:
		pkg_resources.resource_filename('nanometa_live', 'snakemake_envs/blast_validation_env.yaml')
	shell:
		"blastn -db $(dirname {input.database})/$(basename {input.database} .nsq) -query {input.id_fasta} -out {output} -outfmt 6 -perc_identity " + str(config["min_perc_identity"]) + " -evalue " + str(config["e_val_cutoff"])

rule force_blast:
	# Makes BLAST iterate over each ID.
	input:	# ID variable needs to be skipped if validation=False
		id_names = expand(os.path.join(config["main_dir"], "blast_result_files/{ID}.txt"), ID = ids) if config["blast_validation"] else []
	output:
		os.path.join(config["main_dir"], "blast_result_files/force_blast.txt")
	shell:
		"""
		for file in {input.id_names};
		do
		echo $(basename "$file") >> {output};
		done
		"""
